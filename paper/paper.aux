\relax 
\providecommand\hyper@newdestlabel[2]{}
\providecommand\HyperFirstAtBeginDocument{\AtBeginDocument}
\HyperFirstAtBeginDocument{\ifx\hyper@anchor\@undefined
\global\let\oldcontentsline\contentsline
\gdef\contentsline#1#2#3#4{\oldcontentsline{#1}{#2}{#3}}
\global\let\oldnewlabel\newlabel
\gdef\newlabel#1#2{\newlabelxx{#1}#2}
\gdef\newlabelxx#1#2#3#4#5#6{\oldnewlabel{#1}{{#2}{#3}}}
\AtEndDocument{\ifx\hyper@anchor\@undefined
\let\contentsline\oldcontentsline
\let\newlabel\oldnewlabel
\fi}
\fi}
\global\let\hyper@last\relax 
\gdef\HyperFirstAtBeginDocument#1{#1}
\providecommand\HyField@AuxAddToFields[1]{}
\providecommand\HyField@AuxAddToCoFields[2]{}
\babel@aux{english}{}
\@writefile{toc}{\contentsline {section}{\numberline {I}Introduction}{1}{section.1}\protected@file@percent }
\@writefile{lof}{\contentsline {figure}{\numberline {1}{\ignorespaces Basic schematic of the circular flow model, a simple economic model in which society is composed of two types of agents (firms and people) that interact in two distinct markets.\relax }}{1}{figure.caption.1}\protected@file@percent }
\providecommand*\caption@xref[2]{\@setref\relax\@undefined{#1}}
\newlabel{fig:circularflow}{{1}{1}{Basic schematic of the circular flow model, a simple economic model in which society is composed of two types of agents (firms and people) that interact in two distinct markets.\relax }{figure.caption.1}{}}
\citation{RePEc:col:000092:003907}
\citation{RePEc:col:000092:003907}
\citation{pricing-multi-agent}
\citation{vandenBos2137}
\citation{10.1257/aer.96.4.1029}
\citation{10.1257/aer.96.4.1029}
\citation{doi:10.1002/int.20121}
\citation{doi:10.1002/int.20121}
\@writefile{toc}{\contentsline {section}{\numberline {II}Literature Review}{2}{section.2}\protected@file@percent }
\@writefile{toc}{\contentsline {section}{\numberline {III}Methods}{2}{section.3}\protected@file@percent }
\@writefile{toc}{\contentsline {subsection}{\numberline {A}Circular Flow Architecture}{2}{subsection.3.1}\protected@file@percent }
\citation{actor-critic}
\citation{reinforce-lectures}
\citation{actor-critic}
\@writefile{toc}{\contentsline {subsection}{\numberline {B}RL Algorithms}{3}{subsection.3.2}\protected@file@percent }
\citation{reinforce-lectures}
\citation{reinforce-lectures}
\citation{reinforce-lectures}
\citation{actor-critic}
\@writefile{toc}{\contentsline {subsubsection}{\numberline {B.1}REINFORCE Policy Gradient}{4}{subsubsection.3.2.1}\protected@file@percent }
\@writefile{loa}{\contentsline {algorithm}{\numberline {1}{\ignorespaces REINFORCE Policy Gradient \cite  {reinforce-lectures}\relax }}{4}{algorithm.1}\protected@file@percent }
\newlabel{euclid}{{1}{4}{REINFORCE Policy Gradient \cite {reinforce-lectures}\relax }{algorithm.1}{}}
\@writefile{toc}{\contentsline {subsubsection}{\numberline {B.2}Actor-Critic}{4}{subsubsection.3.2.2}\protected@file@percent }
\@writefile{loa}{\contentsline {algorithm}{\numberline {2}{\ignorespaces Q Actor Critic\relax }}{4}{algorithm.2}\protected@file@percent }
\newlabel{euclid}{{2}{4}{Q Actor Critic\relax }{algorithm.2}{}}
\@writefile{toc}{\contentsline {section}{\numberline {IV}Results and Data Analysis}{4}{section.4}\protected@file@percent }
\@writefile{toc}{\contentsline {subsection}{\numberline {A}Preliminary Results}{4}{subsection.4.1}\protected@file@percent }
\@writefile{lof}{\contentsline {figure}{\numberline {2}{\ignorespaces The agents' losses converge to zero over time. This plot illustrates the loss of firm agents vs. human agents in a simulation with one firm agent and five human agents. The loss for the human agents is the average of all five human agents' losses. This simulation uses the REINFORCE learning algorithm, and indicates that the algorithm is succcessfully converging to a policy within 100 episodes.\relax }}{5}{figure.caption.2}\protected@file@percent }
\newlabel{plt:losstozero}{{2}{5}{The agents' losses converge to zero over time. This plot illustrates the loss of firm agents vs. human agents in a simulation with one firm agent and five human agents. The loss for the human agents is the average of all five human agents' losses. This simulation uses the REINFORCE learning algorithm, and indicates that the algorithm is succcessfully converging to a policy within 100 episodes.\relax }{figure.caption.2}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {3}{\ignorespaces Firm profit increases over time. This plot shows the average profit the firm gains per iteration as a function of episode. The simulation shown uses a single firm agent and five human agents, all using the REINFORCE learning algorithm. This indicates that the REINFORCE learning algorithm is yielding higher reward and better policies over time.\relax }}{5}{figure.caption.3}\protected@file@percent }
\newlabel{plt:firmprofitincrease}{{3}{5}{Firm profit increases over time. This plot shows the average profit the firm gains per iteration as a function of episode. The simulation shown uses a single firm agent and five human agents, all using the REINFORCE learning algorithm. This indicates that the REINFORCE learning algorithm is yielding higher reward and better policies over time.\relax }{figure.caption.3}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {B}Investigating Economic Phenomena}{5}{subsection.4.2}\protected@file@percent }
\citation{scipy}
\@writefile{lof}{\contentsline {figure}{\numberline {4}{\ignorespaces With a single firm agent and five human agents, the firm learns to increase the price of goods to maximize its own profit. The bars shown correspond to the frequency at which the firm chooses a given price to offer for the first episode and the last episode. The possible prices are discretized to be in the set $\{1, 4, 7, 10, 13, 16, 19, 22, 25, 28\}$. This behavior is consistent with that of a monopoly.\relax }}{6}{figure.caption.4}\protected@file@percent }
\newlabel{plt:firmactions}{{4}{6}{With a single firm agent and five human agents, the firm learns to increase the price of goods to maximize its own profit. The bars shown correspond to the frequency at which the firm chooses a given price to offer for the first episode and the last episode. The possible prices are discretized to be in the set $\{1, 4, 7, 10, 13, 16, 19, 22, 25, 28\}$. This behavior is consistent with that of a monopoly.\relax }{figure.caption.4}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {5}{\ignorespaces Policy Network Loss in Q Actor Critic Method: with one actor-critic firm agent and five human agents, the firm's policy network loss converges rather quickly to 0 (note that the y-axis increments are rather small). Like in REINFORCE, when the policy becomes close to deterministic, the loss approaches zero.\relax }}{6}{figure.caption.5}\protected@file@percent }
\newlabel{plt:acpolicyloss}{{5}{6}{Policy Network Loss in Q Actor Critic Method: with one actor-critic firm agent and five human agents, the firm's policy network loss converges rather quickly to 0 (note that the y-axis increments are rather small). Like in REINFORCE, when the policy becomes close to deterministic, the loss approaches zero.\relax }{figure.caption.5}{}}
\@writefile{toc}{\contentsline {section}{\numberline {V}Conclusion}{6}{section.5}\protected@file@percent }
\bibstyle{unsrt}
\bibdata{sources}
\bibcite{RePEc:col:000092:003907}{1}
\bibcite{pricing-multi-agent}{2}
\bibcite{vandenBos2137}{3}
\@writefile{lof}{\contentsline {figure}{\numberline {6}{\ignorespaces Critic Network Loss in Q Actor Critic Method: with one actor-critic firm agent and five human agents, the firm's critic loss does not converge, even with changing the learning rate. This was also true when we used A2C, which is supposed to address variance issues with a baseline. Note that the scale on the y-axis is quite large.\relax }}{7}{figure.caption.6}\protected@file@percent }
\newlabel{plt:acadvloss}{{6}{7}{Critic Network Loss in Q Actor Critic Method: with one actor-critic firm agent and five human agents, the firm's critic loss does not converge, even with changing the learning rate. This was also true when we used A2C, which is supposed to address variance issues with a baseline. Note that the scale on the y-axis is quite large.\relax }{figure.caption.6}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {7}{\ignorespaces The money of each human agent during the first episode. All human agents appear to follow greedy policies, purchasing goods as quickly as possible before running out of money. The brighter colors correspond to higher skill levels.\relax }}{7}{figure.caption.7}\protected@file@percent }
\newlabel{plt:init-ppl}{{7}{7}{The money of each human agent during the first episode. All human agents appear to follow greedy policies, purchasing goods as quickly as possible before running out of money. The brighter colors correspond to higher skill levels.\relax }{figure.caption.7}{}}
\@writefile{toc}{\contentsline {section}{\numberline {VI}Acknowledgments}{7}{section.6}\protected@file@percent }
\bibcite{10.1257/aer.96.4.1029}{4}
\bibcite{doi:10.1002/int.20121}{5}
\bibcite{actor-critic}{6}
\bibcite{reinforce-lectures}{7}
\@writefile{lof}{\contentsline {figure}{\numberline {8}{\ignorespaces The money of each firm agent during the first episode. All firms appear to follow policies that are complementary to the greedy policies present in the human agents. Once the human agents run out of money around iteration 100, the firms are no longer profitable.\relax }}{8}{figure.caption.8}\protected@file@percent }
\newlabel{plt:init-firms}{{8}{8}{The money of each firm agent during the first episode. All firms appear to follow policies that are complementary to the greedy policies present in the human agents. Once the human agents run out of money around iteration 100, the firms are no longer profitable.\relax }{figure.caption.8}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {9}{\ignorespaces The money of each human agent during episode 60. The human agents have learned various policies, including a retirement policy in which the agent saves money and then spends money. The brighter colors correspond to higher skill levels.\relax }}{8}{figure.caption.9}\protected@file@percent }
\newlabel{plt:late-ppl}{{9}{8}{The money of each human agent during episode 60. The human agents have learned various policies, including a retirement policy in which the agent saves money and then spends money. The brighter colors correspond to higher skill levels.\relax }{figure.caption.9}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {10}{\ignorespaces The money of each firm agent during episode 60. Despite being identical, the diversity in policies led to some firms generating large profits, while others experienced less success.\relax }}{8}{figure.caption.10}\protected@file@percent }
\newlabel{plt:late-firms}{{10}{8}{The money of each firm agent during episode 60. Despite being identical, the diversity in policies led to some firms generating large profits, while others experienced less success.\relax }{figure.caption.10}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {11}{\ignorespaces Total goods consumed by each human agent during episode 60. The most successful policy corresponds to the human agent in Figure \ref  {plt:late-ppl} which ends with the most money (following a retirement policy).\relax }}{9}{figure.caption.11}\protected@file@percent }
\newlabel{plt:late-goodspeople}{{11}{9}{Total goods consumed by each human agent during episode 60. The most successful policy corresponds to the human agent in Figure \ref {plt:late-ppl} which ends with the most money (following a retirement policy).\relax }{figure.caption.11}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {12}{\ignorespaces The average market price of goods during episode 60. The transparent plot indicates the true values, and the superimposed red plot is a smoothed version.\relax }}{9}{figure.caption.12}\protected@file@percent }
\newlabel{plt:late-stockmarket}{{12}{9}{The average market price of goods during episode 60. The transparent plot indicates the true values, and the superimposed red plot is a smoothed version.\relax }{figure.caption.12}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {13}{\ignorespaces The Gini coefficient during episode 60. The transparent plot indicates the true values, and the superimposed red plot is a smoothed version. The inequality in the population remains generally stable, but tends to trend upwards after 50 iterations.\relax }}{9}{figure.caption.13}\protected@file@percent }
\newlabel{plt:late-gini}{{13}{9}{The Gini coefficient during episode 60. The transparent plot indicates the true values, and the superimposed red plot is a smoothed version. The inequality in the population remains generally stable, but tends to trend upwards after 50 iterations.\relax }{figure.caption.13}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {14}{\ignorespaces The population GDP during episode 60. The transparent plot indicates true values, and the superimposed red plot is a smoothed version.\relax }}{9}{figure.caption.14}\protected@file@percent }
\newlabel{plt:late-gdp}{{14}{9}{The population GDP during episode 60. The transparent plot indicates true values, and the superimposed red plot is a smoothed version.\relax }{figure.caption.14}{}}
