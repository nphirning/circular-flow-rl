@TechReport{RePEc:col:000092:003907,
  author={Fernando Lozano and Jaime Lozano and Mario García},
  title={{An artificial economy based on reinforcement learning and agent based modeling}},
  year=2007,
  month=Apr,
  institution={Universidad del Rosario},
  type={Documentos de Trabajo},
  url={https://ideas.repec.org/p/col/000092/003907.html},
  number={003907},
  abstract={In this paper we employ techniques from artificial intelligence such as reinforcement learning and agent based modeling as building blocks of a computational model for an economy based on convention. First we model the interaction among firms in the private sector. These firms behave in an information environment based on conventions meaning that a firm is likely to behave as it neighbors if it observes that their actions lead to a good pay-off. On the other hand, we propose the use of reinforcement learning as a computational model for the role of goverment in the economy, as the agent that determines the fiscal policy, and whose objective is to maximize economy growth. We present the implementation of a simulator of the proposed model based on SWARM, that employs the SARSA algotithm combined wiht a multilayer perceptron as the function approximation for the action value function},
  keywords={reinforcement learning; agent-based modeling; computational economics},
  doi={},
}

@article{pricing-multi-agent,
author = {Tesauro, Gerald and Kephart, Jeffrey},
year = {1999},
month = {10},
pages = {},
title = {Pricing in Agent Economies Using Multi-Agent Q-Learning},
volume = {5},
journal = {Autonomous Agent and Multi-Agent Systems},
doi = {10.1023/A:1015504423309}
}

@article {vandenBos2137,
	author = {van den Bos, Wouter and Talwar, Arjun and McClure, Samuel M.},
	title = {Neural Correlates of Reinforcement Learning and Social Preferences in Competitive Bidding},
	volume = {33},
	number = {5},
	pages = {2137--2146},
	year = {2013},
	doi = {10.1523/JNEUROSCI.3095-12.2013},
	publisher = {Society for Neuroscience},
	abstract = {In competitive social environments, people often deviate from what rational choice theory prescribes, resulting in losses or suboptimal monetary gains. We investigate how competition affects learning and decision-making in a common value auction task. During the experiment, groups of five human participants were simultaneously scanned using MRI while playing the auction task. We first demonstrate that bidding is well characterized by reinforcement learning with biased reward representations dependent on social preferences. Indicative of reinforcement learning, we found that estimated trial-by-trial prediction errors correlated with activity in the striatum and ventromedial prefrontal cortex. Additionally, we found that individual differences in social preferences were related to activity in the temporal-parietal junction and anterior insula. Connectivity analyses suggest that monetary and social value signals are integrated in the ventromedial prefrontal cortex and striatum. Based on these results, we argue for a novel mechanistic account for the integration of reinforcement history and social preferences in competitive decision-making.},
	issn = {0270-6474},
	URL = {https://www.jneurosci.org/content/33/5/2137},
	eprint = {https://www.jneurosci.org/content/33/5/2137.full.pdf},
	journal = {Journal of Neuroscience}
}

@article{exp-weighted-attr,
author = {Camerer, Colin and Ho, Teck},
year = {1997},
month = {01},
pages = {},
title = {Experience-Weighted Attraction Learning in Games: A Unifying Approach},
journal = {California Institute of Technology, Division of the Humanities and Social Sciences, Working Papers}
}

@article{10.1257/aer.96.4.1029,
Author = {Bereby-Meyer, Yoella and Roth, Alvin E.},
Title = {The Speed of Learning in Noisy Games: Partial Reinforcement and the Sustainability of Cooperation},
Journal = {American Economic Review},
Volume = {96},
Number = {4},
Year = {2006},
Month = {September},
Pages = {1029-1042},
DOI = {10.1257/aer.96.4.1029},
URL = {http://www.aeaweb.org/articles?id=10.1257/aer.96.4.1029}}

@article{doi:10.1002/int.20121,
author = {Könönen, Ville},
title = {Dynamic pricing based on asymmetric multiagent reinforcement learning},
journal = {International Journal of Intelligent Systems},

volume = {21},
number = {1},
pages = {73-98},
doi = {10.1002/int.20121},
url = {https://onlinelibrary.wiley.com/doi/abs/10.1002/int.20121},
eprint = {https://onlinelibrary.wiley.com/doi/pdf/10.1002/int.20121},
abstract = {Abstract A dynamic pricing problem is solved by using asymmetric multiagent reinforcement learning in this article. In the problem, there are two competing brokers that sell identical products to customers and compete on the basis of price. We model this dynamic pricing problem as a Markov game and solve it by using two different learning methods. The first method utilizes modified gradient descent in the parameter space of the value function approximator and the second method uses a direct gradient of the parameterized policy function. We present a brief literature survey of pricing models based on multiagent reinforcement learning, introduce the basic concepts of Markov games, and solve the problem by using proposed methods. © 2006 Wiley Periodicals, Inc. Int J Int Syst 21: 73–98, 2006.},
year = {2006}
}

