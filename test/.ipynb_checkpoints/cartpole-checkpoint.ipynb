{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Policy Gradients on CartPole with PyTorch 1.0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import gym\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torch.nn.functional as F\n",
    "from torch.distributions import Categorical"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "env = gym.make('CartPole-v1')\n",
    "env.seed(1)\n",
    "torch.manual_seed(1)\n",
    "\n",
    "# Hyperparameters\n",
    "learning_rate = 0.01\n",
    "gamma = 0.99"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Define model and training loop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Policy(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(Policy, self).__init__()\n",
    "        state_space = env.observation_space.shape[0]\n",
    "        action_space = env.action_space.n\n",
    "        num_hidden = 128\n",
    "\n",
    "        self.l1 = nn.Linear(state_space, num_hidden, bias=False)\n",
    "        self.l2 = nn.Linear(num_hidden, action_space, bias=False)\n",
    "\n",
    "        # Overall reward and loss history\n",
    "        self.reward_history = []\n",
    "        self.loss_history = []\n",
    "        self.reset()\n",
    "\n",
    "    def reset(self):\n",
    "        # Episode policy and reward history\n",
    "        self.episode_actions = torch.Tensor([])\n",
    "        self.episode_rewards = []\n",
    "\n",
    "    def forward(self, x):\n",
    "        model = torch.nn.Sequential(\n",
    "            self.l1,\n",
    "            nn.Dropout(p=0.5),\n",
    "            nn.Tanh(),\n",
    "            self.l2,\n",
    "            nn.Softmax(dim=-1)\n",
    "        )\n",
    "        return model(x)\n",
    "\n",
    "\n",
    "def predict(state):\n",
    "    # Select an action (0 or 1) by running policy model\n",
    "    # and choosing based on the probabilities in state\n",
    "    state = torch.from_numpy(state).type(torch.FloatTensor)\n",
    "    action_probs = policy(state)\n",
    "    distribution = Categorical(action_probs)\n",
    "    action = distribution.sample()\n",
    "\n",
    "    # Add log probability of our chosen action to our history\n",
    "    policy.episode_actions = torch.cat([\n",
    "        policy.episode_actions,\n",
    "        distribution.log_prob(action).reshape(1)\n",
    "    ])\n",
    "\n",
    "    return action\n",
    "\n",
    "\n",
    "def update_policy():\n",
    "    R = 0\n",
    "    rewards = []\n",
    "\n",
    "    # Discount future rewards back to the present using gamma\n",
    "    for r in policy.episode_rewards[::-1]:\n",
    "        R = r + gamma * R\n",
    "        rewards.insert(0, R)\n",
    "\n",
    "    # Scale rewards\n",
    "    rewards = torch.FloatTensor(rewards)\n",
    "    rewards = (rewards - rewards.mean()) / \\\n",
    "        (rewards.std() + np.finfo(np.float32).eps)\n",
    "\n",
    "    # Calculate loss\n",
    "    loss = (torch.sum(torch.mul(policy.episode_actions, rewards).mul(-1), -1))\n",
    "\n",
    "    # Update network weights\n",
    "    optimizer.zero_grad()\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "\n",
    "    # Save and intialize episode history counters\n",
    "    policy.loss_history.append(loss.item())\n",
    "    policy.reward_history.append(np.sum(policy.episode_rewards))\n",
    "    policy.reset()\n",
    "\n",
    "\n",
    "def train(episodes):\n",
    "    scores = []\n",
    "    for episode in range(episodes):\n",
    "        # Reset environment and record the starting state\n",
    "        state = env.reset()\n",
    "\n",
    "        for time in range(1000):\n",
    "            print(state)\n",
    "            action = predict(state)\n",
    "\n",
    "            # Uncomment to render the visual state in a window\n",
    "            # env.render()\n",
    "\n",
    "            # Step through environment using chosen action\n",
    "            state, reward, done, _ = env.step(action.item())\n",
    "\n",
    "            # Save reward\n",
    "            policy.episode_rewards.append(reward)\n",
    "            if done:\n",
    "                break\n",
    "\n",
    "        update_policy()\n",
    "\n",
    "        # Calculate score to determine when the environment has been solved\n",
    "        scores.append(time)\n",
    "        mean_score = np.mean(scores[-100:])\n",
    "\n",
    "        if episode % 50 == 0:\n",
    "            print('Episode {}\\tAverage length (last 100 episodes): {:.2f}'.format(\n",
    "                episode, mean_score))\n",
    "\n",
    "        if mean_score > env.spec.reward_threshold:\n",
    "            print(\"Solved after {} episodes! Running average is now {}. Last episode ran to {} time steps.\"\n",
    "                  .format(episode, mean_score, time))\n",
    "            break"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Start training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([-0.0261, -0.0006, -0.0198, -0.0280])\n",
      "tensor([-0.0261,  0.1948, -0.0204, -0.3269])\n",
      "tensor([-2.2192e-02,  1.3110e-05, -2.6891e-02, -4.0660e-02])\n",
      "tensor([-0.0222,  0.1955, -0.0277, -0.3417])\n",
      "tensor([-0.0183,  0.0008, -0.0345, -0.0579])\n",
      "tensor([-0.0183, -0.1938, -0.0357,  0.2237])\n",
      "tensor([-0.0221, -0.3884, -0.0312,  0.5049])\n",
      "tensor([-0.0299, -0.5831, -0.0211,  0.7876])\n",
      "tensor([-0.0416, -0.7779, -0.0054,  1.0736])\n",
      "tensor([-0.0571, -0.5827,  0.0161,  0.7792])\n",
      "tensor([-0.0688, -0.7781,  0.0317,  1.0769])\n",
      "tensor([-0.0843, -0.5834,  0.0532,  0.7943])\n",
      "tensor([-0.0960, -0.3890,  0.0691,  0.5189])\n",
      "tensor([-0.1038, -0.1949,  0.0795,  0.2487])\n",
      "tensor([-0.1077, -0.0010,  0.0845, -0.0179])\n",
      "tensor([-0.1077, -0.1972,  0.0841,  0.3002])\n",
      "tensor([-0.1117, -0.0034,  0.0901,  0.0352])\n",
      "tensor([-0.1117,  0.1903,  0.0908, -0.2277])\n",
      "tensor([-0.1079,  0.3840,  0.0863, -0.4905])\n",
      "tensor([-0.1002,  0.5778,  0.0764, -0.7548])\n",
      "tensor([-0.0887,  0.3817,  0.0614, -0.4390])\n",
      "tensor([-0.0810,  0.1858,  0.0526, -0.1277])\n",
      "tensor([-0.0773, -0.0100,  0.0500,  0.1811])\n",
      "tensor([-0.0775,  0.1843,  0.0536, -0.0954])\n",
      "tensor([-0.0738,  0.3787,  0.0517, -0.3706])\n",
      "tensor([-0.0663,  0.5730,  0.0443, -0.6466])\n",
      "tensor([-0.0548,  0.3773,  0.0314, -0.3403])\n",
      "tensor([-0.0473,  0.1817,  0.0246, -0.0379])\n",
      "tensor([-0.0436,  0.3765,  0.0238, -0.3227])\n",
      "tensor([-0.0361,  0.1810,  0.0174, -0.0226])\n",
      "tensor([-0.0325,  0.3759,  0.0169, -0.3097])\n",
      "tensor([-0.0250,  0.5708,  0.0107, -0.5970])\n",
      "tensor([-0.0135,  0.3755, -0.0012, -0.3010])\n",
      "tensor([-0.0060,  0.5707, -0.0072, -0.5941])\n",
      "tensor([ 0.0054,  0.7659, -0.0191, -0.8890])\n",
      "tensor([ 0.0207,  0.9613, -0.0369, -1.1876])\n",
      "tensor([ 0.0399,  0.7666, -0.0606, -0.9068])\n",
      "tensor([ 0.0553,  0.5724, -0.0788, -0.6337])\n",
      "tensor([ 0.0667,  0.7685, -0.0915, -0.9501])\n",
      "tensor([ 0.0821,  0.9647, -0.1105, -1.2701])\n",
      "tensor([ 0.1014,  0.7712, -0.1359, -1.0140])\n",
      "tensor([ 0.1168,  0.9678, -0.1561, -1.3460])\n",
      "tensor([ 0.1361,  1.1645, -0.1831, -1.6832])\n",
      "Episode 0\tAverage length (last 100 episodes): 42.00\n"
     ]
    }
   ],
   "source": [
    "policy = Policy()\n",
    "optimizer = optim.Adam(policy.parameters(), lr=learning_rate)\n",
    "train(episodes=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Plot training performance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# number of episodes for rolling average\n",
    "window = 50\n",
    "\n",
    "fig, ((ax1), (ax2)) = plt.subplots(2, 1, sharey=True, figsize=[9, 9])\n",
    "rolling_mean = pd.Series(policy.reward_history).rolling(window).mean()\n",
    "std = pd.Series(policy.reward_history).rolling(window).std()\n",
    "ax1.plot(rolling_mean)\n",
    "ax1.fill_between(range(len(policy.reward_history)), rolling_mean -\n",
    "                 std, rolling_mean+std, color='orange', alpha=0.2)\n",
    "ax1.set_title(\n",
    "    'Episode Length Moving Average ({}-episode window)'.format(window))\n",
    "ax1.set_xlabel('Episode')\n",
    "ax1.set_ylabel('Episode Length')\n",
    "\n",
    "ax2.plot(policy.reward_history)\n",
    "ax2.set_title('Episode Length')\n",
    "ax2.set_xlabel('Episode')\n",
    "ax2.set_ylabel('Episode Length')\n",
    "\n",
    "fig.tight_layout(pad=2)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "file_extension": ".py",
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  },
  "mimetype": "text/x-python",
  "name": "python",
  "npconvert_exporter": "python",
  "pygments_lexer": "ipython3",
  "version": 3
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
